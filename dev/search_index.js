var documenterSearchIndex = {"docs":
[{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"EditURL = \"../../../examples/mcmc.jl\"","category":"page"},{"location":"generated/mcmc_lit/#Load-packages","page":"Hyperparameter inference","title":"Load packages","text":"","category":"section"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"using Pkg\nPkg.activate(\".\")\nPkg.Registry.add(RegistrySpec(url = \"https://github.com/RoyCCWang/RWPublicJuliaRegistry\")) # where MaternRegression.jl is registered.\nlet\n    pkgs = [\"StaticArrays\", \"MaternRegression\", \"CSV\", \"DataFrames\", \"PythonPlot\",\n    \"FiniteDiff\", \"SimpleUnPack\", \"TransformVariables\", \"TransformedLogDensities\",\n    \"LogDensityProblems\", \"DynamicHMC\"]\n    for pkg in pkgs\n        #check if package is in current environment.\n        if Base.find_package(pkg) === nothing\n\n            #install package.\n            Pkg.add(pkg)\n        end\n    end\nend\n\nimport Random\nRandom.seed!(25)\n\nusing LinearAlgebra\nusing Statistics\nimport MaternRegression as GS;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"  Activating project at `~/Documents/repo/MaternRegression.jl/examples`\n     Cloning registry from \"https://github.com/RoyCCWang/RWPublicJuliaRegistry\"\nRegistry `RWPublicJuliaRegistry` already exists in `~/.julia/registries/RWPublicJuliaRegistry`.\n","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"When we import the following, the add-on module in MaternRegression for hyperparameter inference also gets loaded.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"import FiniteDiff as SFD\nusing SimpleUnPack\nimport TransformVariables as TV\nimport TransformedLogDensities as TD\nimport LogDensityProblems\nimport DynamicHMC as HMC;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"for loading the data.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"import CSV\nimport DataFrames as DF\nimport Dates;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Reset plot figures.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"import PythonPlot as PLT\nPLT.close(\"all\")\nfig_num = 1;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Specify floating-point data type.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"T = Float64;","category":"page"},{"location":"generated/mcmc_lit/#Load-data","page":"Hyperparameter inference","title":"Load data","text":"","category":"section"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Get all the data from the csv into a data frame.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"function get_toronto_station()\n    return \"CA006158665\"\nend\n\nfunction load_data(station_name)\n    data_path = joinpath(\"data\", \"$(station_name).csv\")\n    return CSV.read(data_path, DF.DataFrame)\nend\n\nfunction reverse_standardization(yq::AbstractVector, m, s)\n    return collect(s*yq[n] + m for n in eachindex(yq))\nend\n\ndf_all = load_data(get_toronto_station());","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Remove daily records that have missing maximum temperature.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"df_tmax = filter(xx->!ismissing(xx.TMAX), df_all);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"The temperature measurements needs to be divided by 10 to get Celcius units.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"N = length(df_tmax.TMAX )\ny0 = collect( convert(T, x/10) for x in df_tmax.TMAX );","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"y is the set of raining outputs.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"mean_y = mean(y0)\nstd_y = std(y0)\ny = (y0 .- mean_y) ./ std_y;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Convert dates to integers, and use as training inputs.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"ts_dates = df_tmax.DATE\nts0 = collect( convert(T,  d |> Dates.value) for d in ts_dates );","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Work with elapsed days as the independent variable. ts is the set of training inputs.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"ts = ts0 .- minimum(ts0)\noffset_date = ts_dates[begin];","category":"page"},{"location":"generated/mcmc_lit/#Hyperparameter-inference","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"","category":"section"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Specify common hyperparameters for the inverse gamma prior for λ, σ², b.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"α = convert(T, 1e-3)\nβ = convert(T, 1e-3);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Number of MCMC draws. Reduce N_draws if the inference takes too long on your computer.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"N_draws = 1_000;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Run inference.You need to pass in the module alias HMC so that MaternRegression can check if the pre-requisite dependencies for the hyperparameter inference package extension can be loaded.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"model_trait = GS.InferGain()\nλ_samples, σ²_samples, b_samples, mcmc_results = GS.hp_inference(\n    GS.UseDynamicHMC([SFD; SimpleUnPack; TV; TD; LogDensityProblems; HMC]),\n    model_trait,\n    N_draws, α, β, ts, y,\n);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"If you wish to save the results to disk, then uncomment the following:","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"##Save to disk, for later use with the QMD file.\n#using Serialization\n#serialize(\"results/mcmc\", (λ_samples, σ²_samples, b_samples, mcmc_results));","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"To load results from disk, uncomment the following:","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"##Load from disk, make sure there were no errors in loading.\n#using Serialization\n#λ_samples, σ²_samples, b_samples, mcmc_results = deserialize(\"results/mcmc\");","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"The marginal empirical mean and variance of the drawn samples.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"λ_m, λ_v = mean(λ_samples), var(λ_samples)\nσ²_m, σ²_v = mean(σ²_samples), var(σ²_samples)\nb_m, b_v = mean(b_samples), var(b_samples);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Check out the Hamiltonian Monte Carlo diagnostics.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"HMC.Diagnostics.summarize_tree_statistics(mcmc_results.tree_statistics)","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Hamiltonian Monte Carlo sample of length 1000\n  acceptance rate mean: 0.93, 5/25/50/75/95%: 0.72 0.9 0.97 0.99 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 2%, 2 => 40%, 3 => 47%, 4 => 11%","category":"page"},{"location":"generated/mcmc_lit/#Marginal-Histograms","page":"Hyperparameter inference","title":"Marginal Histograms","text":"","category":"section"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"nbins = 200","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"200","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"The marginal λ samples.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"PLT.figure(fig_num)\nfig_num += 1\nPLT.hist(λ_samples, nbins)\nPLT.title(\"Marginal histogram, λ\")\nPLT.show() #Not needed if run in the Julia REPL.\nPLT.gcf() #required for Literate.jl","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"(Image: )","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"The marginal σ² samples.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"PLT.figure(fig_num)\nfig_num += 1\nPLT.hist(σ²_samples, nbins)\nPLT.title(\"Marginal histogram, σ²\")\nPLT.show() #Not needed if run in the Julia REPL.\nPLT.gcf() #required for Literate.jl","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"(Image: )","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"The marginal b samples.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"PLT.figure(fig_num)\nfig_num += 1\nPLT.hist(b_samples, nbins)\nPLT.title(\"Marginal histogram, b\")\nPLT.show() #Not needed if run in the Julia REPL.\nPLT.gcf() #required for Literate.jl","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"(Image: )","category":"page"},{"location":"generated/mcmc_lit/#Query-inferred-model","page":"Hyperparameter inference","title":"Query inferred model","text":"","category":"section"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"We now draw samples from an ensemble of GPR models, each GPR model corresponds to a drawn hyperparameter sample from the hyperparameter inference.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Reset seed for reproducibility.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Random.seed!(25)","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Random.TaskLocalRNG()","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Choose 5000 uniformly spaced time stamps across the in-fill interval, then pick the first window_len of them as the query positions.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Nq = 5000\nwindow_len = 50\ntqs = LinRange(ts[begin], ts[end], Nq)[1:window_len];","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"We'll draw M number of samples per GPR model.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"M = 1_000\nquery_samples = GS.simulate_sdegps(\n    λ_samples, σ²_samples, b_samples, M, ts, tqs, y,\n);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"If you wish to save the results to disk, then uncomment the following:","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"##Save to disk, for later use with the QMD file.\n#using Serialization\n#serialize(\"results/mcmc_query\", query_samples)","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"To load results from disk, uncomment the following:","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"##Load from disk, make sure there were no errors in loading.\n#using Serialization\n#query_samples = deserialize(\"results/mcmc_query\")","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Separate into predictive means mqs and variances vqs.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"mqs, vqs = GS.compute_mean_var(query_samples);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Get the preditive standard deviation.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"sqs = sqrt.(vqs);","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"prepare for display.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"inds = findall(xx->xx<tqs[end], ts)\nts_display = ts[inds]\ny_display = reverse_standardization(y[inds], mean_y, std_y)\n\nmqs = reverse_standardization(mqs, mean_y, std_y)\nsqs = sqs .* std_y;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Set the shaded region to be 3 standard deviations from the mean.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"plot_uq_amount = 3 .* sqs;","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"Visualize.","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"fig_size = (6, 4) # units are in inches.\ndpi = 300\n\nPLT.figure(fig_num; figsize = fig_size, dpi = dpi)\nfig_num += 1\n\nPLT.scatter(ts_display, y_display, s = 20, label = \"Data\")\nPLT.plot(tqs, mqs, label = \"Predictive mean\")\n\nPLT.fill_between(\n    tqs,\n    mqs - plot_uq_amount,\n    mqs + plot_uq_amount,\n    alpha = 0.3,\n    label = \"3 standard deviations\"\n)\n\nPLT.xlabel(\"Days elapsed since $offset_date\", fontsize = 12)\nPLT.ylabel(\"Temperature (Celcius)\", fontsize = 12)\nPLT.legend()\nPLT.show() #Not needed if run in the Julia REPL.\nPLT.gcf() #required for Literate.jl","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"(Image: )","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"","category":"page"},{"location":"generated/mcmc_lit/","page":"Hyperparameter inference","title":"Hyperparameter inference","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"EditURL = \"../../../examples/sde_gp.jl\"","category":"page"},{"location":"generated/sde_gp_lit/#Load-packages","page":"Basic query","title":"Load packages","text":"","category":"section"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"using Pkg\nPkg.activate(\".\")\nPkg.Registry.add(RegistrySpec(url = \"https://github.com/RoyCCWang/RWPublicJuliaRegistry\")) # where MaternRegression.jl is registered.\nlet\n    pkgs = [\"RKHSRegularization\", \"StaticArrays\", \"MaternRegression\", \"PythonPlot\",\n    ]\n    for pkg in pkgs\n        #check if package is in current environment.\n        if Base.find_package(pkg) === nothing\n\n            #install package.\n            Pkg.add(pkg)\n        end\n    end\nend\n\nimport Random\nRandom.seed!(25)\n\nusing LinearAlgebra\nusing Statistics\nimport RKHSRegularization as RK\nimport MaternRegression as GS;\n\ninclude(\"helpers/gpr.jl\")\ninclude(\"helpers/utils.jl\");","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"  Activating project at `~/Documents/repo/MaternRegression.jl/examples`\n     Cloning registry from \"https://github.com/RoyCCWang/RWPublicJuliaRegistry\"\nRegistry `RWPublicJuliaRegistry` already exists in `~/.julia/registries/RWPublicJuliaRegistry`.\n","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Reset plot figures.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"import PythonPlot as PLT\nPLT.close(\"all\")\nfig_num = 1;","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"    CondaPkg Found dependencies: /home/roy/.julia/packages/PythonCall/bb3ax/CondaPkg.toml\n    CondaPkg Found dependencies: /home/roy/.julia/packages/PythonPlot/KcWMF/CondaPkg.toml\n    CondaPkg Dependencies already up to date\n","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Specify floating-point data type.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"T = Float64;","category":"page"},{"location":"generated/sde_gp_lit/#Oracle-model-set-up","page":"Basic query","title":"Oracle model set up","text":"","category":"section"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Specify the Matern 3/2 covariance function hyperparameters.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"λ = convert(T, 1.23)\nb = convert(T, 5.6)\nq_Matern32 = 4*b*λ^3\nθ = RK.Matern3Halfs(λ, b);","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Specify observation variance. Set this to a very small number like 1e-12 for a noise-less regression.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"σ² = convert(T, 1e-6);","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Set up oracle, f.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"N_on_grid = 7\nN_randomly_generated = 9\nub = abs(randn(T))\nlb = -ub\nts = generate_almost_grid(N_on_grid, N_randomly_generated, lb, ub)\n\nf = xx->sinc(4*xx)*xx^3\ny = collect(\n  f(t) + randn(T) * sqrt(σ²)\n  for t in ts\n);","category":"page"},{"location":"generated/sde_gp_lit/#Query","page":"Basic query","title":"Query","text":"","category":"section"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Nq = 1000\ntqs = LinRange(lb, ub, Nq);","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Oracle query.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"f_xq = f.(tqs);","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Fit and query the GPR via RHKSRegularization.jl, which uses a conventional implementation without Bayesian filtering.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"mqs, vqs, η = querygpr(θ, tqs, ts, y, σ²) #predictive mean, prod. variance, GPR model container.\nsqs = sqrt.(vqs); #preditive standard deviation","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Visualize the oracle and (conventional implementation) GPR query results","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"PLT.figure(fig_num)\nfig_num += 1\n\nPLT.scatter(ts, y, s = 20, label = \"Data\")\nPLT.plot(tqs, mqs, label = \"Predictive mean\")\n\nPLT.plot(tqs, f_xq, label = \"oracle\")\nPLT.fill_between(tqs, mqs - sqs, mqs + sqs, alpha = 0.3)\n\nPLT.title(\"Conventional GPR\")\nPLT.legend()\nPLT.show() #Not needed if running from a Julia REPL.\nPLT.gcf() #Required for Literate.jl","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"(Image: )","category":"page"},{"location":"generated/sde_gp_lit/#Query-Comparison","page":"Basic query","title":"Query Comparison","text":"","category":"section"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Conventional implementation of univariate Matern 3/2 GPRs for in-fill problems compared against the stochastic differential equation (SDE) / Bayesian filtering implementation: Set the query positions to be a uniformly spaced grid of Nq samples.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Nq = 5_000\ntqs = LinRange(lb, ub, Nq);","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Conventional GPR.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"θ = RK.Matern3Halfs(λ, b) #covariance function variable from `RKHSRegularization.jl`.\nmqs, vqs, η = querygpr(θ, tqs, ts, y, σ²);","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"SDE GPR.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"θ_sde = GS.create_Matern3HalfsKernel(λ, b) #covariance function variable.\nsde_gp = GS.create_sdegp(θ_sde, ts, y, σ²) #cache phase.\nmqs2, vqs2 = GS.query_sdegp(T, sde_gp, tqs, ts); #query phase.","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Relative discrepancy","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"@show norm(mqs - mqs2)/norm(mqs2)\n@show norm(vqs - vqs2)/norm(vqs2)","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"7.031627432571349e-12","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Marginal likelihood: conventional GPR implementation:","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"ML_gpr = evalnlli_gpr(η, y)","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"90.39341339598067","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"Marginal likelihood: SDE/Bayesian filtering implementation:","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"θ_dummy = GS.create_Matern3HalfsKernel(T)\nML_buffer = GS.setup_ml(θ_dummy, ts, y)\nML_sde = GS.eval_ml!(ML_buffer, θ_sde, ts, y, σ²)","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"90.39341339607462","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"","category":"page"},{"location":"generated/sde_gp_lit/","page":"Basic query","title":"Basic query","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#Function-References","page":"Public API","title":"Function References","text":"","category":"section"},{"location":"api/#Core-functions","page":"Public API","title":"Core functions","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"TimeVarType\ncreate_Matern3HalfsKernel\ncreate_sdegp\nquery_sdegp\nquery_sdegp!\nsetup_ml\neval_ml\neval_ml!","category":"page"},{"location":"api/#MaternRegression.TimeVarType","page":"Public API","title":"MaternRegression.TimeVarType","text":"const TimeVarType = Union{AbstractVector, AbstractRange}\n\nA supertype for specifying the set of training inputs.\n\n\n\n\n\n","category":"type"},{"location":"api/#MaternRegression.create_Matern3HalfsKernel","page":"Public API","title":"MaternRegression.create_Matern3HalfsKernel","text":"create_Matern3HalfsKernel(λ::T, b::T)::Matern3HalfsKernel{T} where T <: AbstractFloat\n\nCreates a Matern covariance function variable with parameters λ and b. Given two time inputs x and z, the distance between two time inputs, this covariance function has the following formula:\n\noutput = b*(1 + λ*norm(x-z))*exp(-λ*norm(x-z))\n\n\n\n\n\ncreate_Matern3HalfsKernel(λ::T)::Matern3HalfsKernel{T} where T <: AbstractFloat\n\nReturns create_Matern3HalfsKernel(λ, one(T))\n\n\n\n\n\ncreate_Matern3HalfsKernel(::Type{T})::Matern3HalfsKernel{T} where T <: AbstractFloat\n\nReturns a template or dummy variable by calling create_Matern3HalfsKernel(one(T), one(T))\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.create_sdegp","page":"Public API","title":"MaternRegression.create_sdegp","text":"create_sdegp(\n    θ_sde::Kernel1D,\n    ts::TimeVarType,\n    y::Vector{T},\n    σ²::T,\n) where T <: Real\n\nTo run Gaussian process query via the state-space implementation, one needs to do a cache phase then a query phase. This function runs the cache phase.\n\nInputs:\n\nθ_sde is a Matern kernel variable\nts is the set of training inputs.\ny is an array of training outputs.\nσ² is the Gaussian process regression's observation variance.\n\nReturns a variable of SDEGP data type, which is used in the query phase.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.query_sdegp","page":"Public API","title":"MaternRegression.query_sdegp","text":"query_sdegp(\n    S::SDEGP,\n    tqs,\n    ts,\n) where T <: Real\n\nThis function allocates the output buffers mqs and vqs, then calls query_sdegp!. See query_sdegp! for details on the inputs.\n\nReturns mqs and vqs.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.query_sdegp!","page":"Public API","title":"MaternRegression.query_sdegp!","text":"query_sdegp!(\n    mqs::Vector{T}, # mutates, output.\n    vqs::Vector{T}, # mutates, output.\n    S::SDEGP,\n    tqs,\n    ts::TimeVarType,\n) where T <: Real\n\nTo run Gaussian process query via the state-space implementation, one needs to do a cache phase then a query phase. This function runs the query phase.\n\nInputs:\n\nmqs Buffer that is mutated and stores the queried predictive means corresponding to entries in tqs.\nvqs Buffer that is mutated and stores the queried predictive variances corresponding to entries in tqs.\nS is the output of the cached phase; see create_sdegp.\ntqs is the set of query inputs.\nts is the set of training inputs.\n\nThe size of mqs and vqs must be the same as tqs.\n\nts should be the same training inputs used to create the cache S, otherwise S needs to be recomputed via create_sdegp.\n\nReturns nothing.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.setup_ml","page":"Public API","title":"MaternRegression.setup_ml","text":"setup_ml(\n    θ_sde::Kernel1D,\n    ts::TimeVarType,\n    y::Vector{T};\n    σ² = one(T),\n) where T <: Real\n\nReturns a buffer variable of type MLBuffers for use with eval_ml!, which computes the marginal likelihood. This avoids additional creation and allocation of this buffer variable if the marginal likelihood is to be computed multiple times for different hyperparameters.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.eval_ml","page":"Public API","title":"MaternRegression.eval_ml","text":"eval_ml(\n    trait::GainTrait,\n    p::Vector{T},\n    ts::Union{AbstractRange, AbstractVector},\n    y::Vector;\n    zero_tol = eps(T)*100,\n) where T <: AbstractFloat\n\nInputs:\n\ntrait is a trait variable that specifies the order of the hyperparameters in p. See trait-based dispatch in the Julia documentation. If typeof(trait) <: InferGain, then the hyperparameters in p are ordered [λ; σ²; b]. If typeof(trait) <: UnityGain, then the hyperparameters in p are ordered [λ; σ²], and b is set to 1.\np is an ordered set of hyperparameter as an array.\nts is the set of training inputs.\ny is the set of training outputs.\nzero_tol needs to be a small positive number. It is the lower bound on the covariance function hyperparameters.\n\nReturns the log marginal likelihood over the training set. Some additive constants might be dropped.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.eval_ml!","page":"Public API","title":"MaternRegression.eval_ml!","text":"eval_ml!(\n    B::MLBuffers, # mutates, buffer\n    θ_sde::Kernel1D,\n    ts::TimeVarType,\n    y::Vector,\n    σ²::Real,\n)\n\nInputs:\n\nbuffer is the return variable from setup_ml.\nθ_sde is the covariance function variable.\nts is the set of training inputs.\ny is the set of training outputs.\nσ² is the observation noise variance. Increase this if you experience numerical issues.\n\nReturns the log marginal likelihood over the training set. Additive constants are dropped.\n\n\n\n\n\neval_ml!(\n    trait::GainTrait,\n    buffer::MLBuffers,\n    p::Vector{T},\n    ts::TimeVarType,\n    y::Vector{T};\n    zero_tol = eps(T)*100,\n) where T <: AbstractFloat\n\nInputs:\n\ntrait is a trait variable that specifies the order of the hyperparameters in p. See trait-based dispatch in the Julia documentation. If typeof(trait) <: InferGain, then the hyperparameters in p are ordered [λ; σ²; b]. If typeof(trait) <: UnityGain, then the hyperparameters in p are ordered [λ; σ²], and b is set to 1.\nbuffer is the return variable from setup_ml.\np is an ordered set of hyperparameter as an array.\nts is the set of training inputs.\ny is the set of training outputs.\nzero_tol needs to be a small positive number. It is the lower bound on the covariance function hyperparameters.\n\nReturns the log marginal likelihood over the training set. Some additive constants might be dropped.\n\n\n\n\n\neval_ml!(\n    buffer::MLBuffers,\n    λ::T,\n    σ²::T,\n    ts::TimeVarType,\n    y::Vector{T};\n    zero_tol = eps(T)*100,\n) where T <: Real\n\nInputs:\n\nbuffer is the return variable from setup_ml.\nλ and σ² are hyperparameters. The b hyperparameter is set to 1.\nts is the set of training inputs.\ny is the set of training outputs.\nzero_tol needs to be a small positive number. It is the lower bound on the covariance function hyperparameters.\n\nReturns the log marginal likelihood over the training set. Some additive constants might be dropped.\n\n\n\n\n\neval_ml!(\n    buffer::MLBuffers,\n    λ::T,\n    σ²::T,\n    b::T,\n    ts::TimeVarType,\n    y::Vector{T};\n    zero_tol = eps(T)*100,\n) where T <: Real\n\nInputs:\n\nbuffer is the return variable from setup_ml.\nλ, σ², and b are hyperparameters.\nts is the set of training inputs.\ny is the set of training outputs.\nzero_tol needs to be a small positive number. It is the lower bound on the covariance function hyperparameters.\n\nReturns the log marginal likelihood over the training set. Some additive constants might be dropped.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hyperparameter-optimization","page":"Public API","title":"Hyperparameter optimization","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"GainTrait\nInferGain\nUnityGain\nhp_optim\nUseMetaheuristics\nparse_ml_result","category":"page"},{"location":"api/#MaternRegression.GainTrait","page":"Public API","title":"MaternRegression.GainTrait","text":"abstract type GainTrait end\n\nSubtypes are InferGain and UnityGain\n\n\n\n\n\n","category":"type"},{"location":"api/#MaternRegression.InferGain","page":"Public API","title":"MaternRegression.InferGain","text":"struct InferGain <: GainTrait end\n\nThis trait specifies that the hyperparameter ordering of the parameter array is λ, σ², b.\n\n\n\n\n\n","category":"type"},{"location":"api/#MaternRegression.UnityGain","page":"Public API","title":"MaternRegression.UnityGain","text":"struct UnityGain <: GainTrait end\n\nThis trait specifies that the hyperparameter ordering of the parameter array is λ, σ². The b hyperparameter is fixed at 1.\n\n\n\n\n\n","category":"type"},{"location":"api/#MaternRegression.hp_optim","page":"Public API","title":"MaternRegression.hp_optim","text":"hp_optim(\n    alg_trait::UseMetaheuristics,\n    model_trait::GainTrait,\n    ts::TimeVarType,\n    y::Vector,\n    lbs::Vector{T},\n    ubs::Vector{T};\n    f_calls_limit = 10_000,\n    p0s::Vector{Vector{T}} = generate_grid(model_trait, lbs, ubs, 10),\n)\n\nChecks if the weak dependencies are loaded in the user's working scope, then checks if the corresponding package extensions are loaded. If so, call the appropriate hyperparameter optimization routine from the package extension.\n\nalg_trait: see UseMetaheuristics.\nmodel_trait is a trait variable that specifies the order of the hyperparameters in p. See trait-based dispatch in the Julia documentation. If typeof(trait) <: InferGain, then the hyperparameters in p are ordered [λ; σ²; b]. If typeof(trait) <: UnityGain, then the hyperparameters in p are ordered [λ; σ²], and b is set to 1. p is used internally by hy_optim.\nts is the set of training inputs.\ny is the set of training outputs.\nlbs and ubs are lower and upper bounds for the hyperparameter vector, p. They follow the ordering as specified by model_trait.\nf_calls_limit is a soft upperbound on the number of marginal likelihood evaluations used during optimization such that the evolutionary algorithm in Metaheuristics.jl tries to not exceed.\np0s is a nested Vector array that contains hyperparameter states that you want to force the optimization algorithm to evaluate. These states can be interpreted as initial guesses to the solution. The default creates a uniform grid of 10x10x10 (if typeof(model_trait) <: InferGain) or 10x10 (if typeof(model_trait) <: UnityGain) from the lower and upper bounds specified.\n\nSee the tutorial for the return type and subsequent usage.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.UseMetaheuristics","page":"Public API","title":"MaternRegression.UseMetaheuristics","text":"struct UseMetaheuristics <: ExtensionPkgs\n    pkg::Module\nend\n\nContainer for verifying whether the weak dependencies for MetaheuristicsExt.jl is loaded in the user's working scope.\n\nThe only dependency required to be loaded in the user's working scope is Metaheuristics.\n\n\n\n\n\n","category":"type"},{"location":"api/#MaternRegression.parse_ml_result","page":"Public API","title":"MaternRegression.parse_ml_result","text":"parse_ml_result(trait::GainTrait, p::Vector{T}) where T <: AbstractFloat\n\nReturns a Matern covariance function variable with the hyperparameters specified in p. The ordering of the hyperparameters in p is specified by trait.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hyperparameter-inference","page":"Public API","title":"Hyperparameter inference","text":"","category":"section"},{"location":"api/","page":"Public API","title":"Public API","text":"hp_inference\nUseDynamicHMC\nsimulate_sdegps\ncompute_mean_var","category":"page"},{"location":"api/#MaternRegression.hp_inference","page":"Public API","title":"MaternRegression.hp_inference","text":"hp_inference(\n    alg_trait::UseDynamicHMC,\n    gain_trait::GainTrait,\n    N_draws::Integer,\n    α::Real,\n    β::Real,\n    ts::TimeVarType,\n    y::Vector,\n)\n\nChecks if the weak dependencies are loaded in the user's working scope, then checks if the corresponding package extensions are loaded. If so, call the appropriate hyperparameter inference routine from the package extension.\n\nalg_trait: see UseDynamicHMC.\ngain_trait is a trait variable that specifies the order of the hyperparameters in p. See trait-based dispatch in the Julia documentation. If typeof(trait) <: InferGain, then the hyperparameters in p are ordered [λ; σ²; b]. If typeof(trait) <: UnityGain, then the hyperparameters in p are ordered [λ; σ²], and b is set to 1. p is used internally by hy_optim.\nα and β are the shared hyperparameters of the inverse gamma prior for the hyperparameters.\nts is the set of training inputs.\ny is the set of training outputs.\n\nSee the tutorial for the return type and subsequent usage.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.UseDynamicHMC","page":"Public API","title":"MaternRegression.UseDynamicHMC","text":"struct UseDynamicHMC <: ExtensionPkgs\n    pkg_list::Vector{Module}\nend\n\nContainer for verifying whether the weak dependencies for DynamicHMCExt.jl is loaded in the user's working scope.\n\nThe dependencies are:\n\nFiniteDiff\nSimpleUnPack\nTransformVariables\nTransformedLogDensities\nLogDensityProblems\nDynamicHMC\n\n\n\n\n\n","category":"type"},{"location":"api/#MaternRegression.simulate_sdegps","page":"Public API","title":"MaternRegression.simulate_sdegps","text":"simulate_sdegps(\n    λ_set::Vector{T},\n    σ²_set::Vector{T},\n    M::Integer,\n    ts::TimeVarType,\n    tqs,\n    y::Vector,\n) where T <: Real\n\nReturns the drawn samples of the ensemble of Gaussian process models that are specified by λ_set and σ²_set. The number of ensemble models is the length of λ_set.\n\nThe gain b is set to 1 for the simulation.\n\nInputs:\n\nλ_set contain samples of the λ parameter. Same length as σ²_set.\nσ²_set contain samples of the σ² parameter.\nM is the number of samples simulate_sdegps simulates per model.\nts is the ordered set of training inputs.\ntqs is the ordered set of query inputs.\ny is the ordered set of training outputs.\n\nThe output, S, is a M x N x K array, where N is the number of ensemble models, and K is the number of query inputs.\n\n\n\n\n\nsimulate_sdegps(\n    λ_set::Vector{T},\n    σ²_set::Vector{T},\n    b_set::Vector{T},\n    M::Integer,\n    ts::TimeVarType,\n    tqs,\n    y::Vector,\n) where T <: Real\n\nReturns the drawn samples of the ensemble of Gaussian process models that are specified by λ_set, σ²_set, and b_set. The number of ensemble models is the length of λ_set.\n\nInputs:\n\nλ_set contain samples of the λ parameter. Same length as σ²_set.\nσ²_set contain samples of the σ² parameter.\nb_set contain samples of the b parameter.\nM is the number of samples simulate_sdegps simulates per model.\nts is the ordered set of training inputs.\ntqs is the ordered set of query inputs.\ny is the ordered set of training outputs.\n\nThe output, S, is a M x N x K array, where N is the number of ensemble models, and K is the number of query inputs.\n\n\n\n\n\n","category":"function"},{"location":"api/#MaternRegression.compute_mean_var","page":"Public API","title":"MaternRegression.compute_mean_var","text":"compute_mean_var(S::Array{T,3}) where T <: Real\n\nAssumes S is a M x N x K array of drawn samples, where:\n\nM is the number of samples drawn from a model.\nN is the number of models.\nK is the number of query positions.\n\ncompute_mean_var computes the empirical mean and variance for each query position.\n\nOutputs:\n\nmqs the empirical means. Length K.\nvqs the empirical variances. Length K.\n\n\n\n\n\n","category":"function"},{"location":"#MaternRegression.jl","page":"Overview","title":"MaternRegression.jl","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"This package implements the state-space model for efficient univariate Gaussian process regression inference for large-scale in-fill problems with a Matérn 32 covariance function. Hyperparameter optimization and inference functionalities are available via the Julia Package Extension framework.","category":"page"},{"location":"#Citation","page":"Overview","title":"Citation","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The companion publication for this package is under peer-review. For now, you can cite this repository and code via the Cite This Repository button on the GitHub webpage of this repository.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"EditURL = \"../../../examples/optim.jl\"","category":"page"},{"location":"generated/optim_lit/#Load-packages","page":"Hyperparameter optimization","title":"Load packages","text":"","category":"section"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"using Pkg\nPkg.activate(\".\")\nPkg.Registry.add(RegistrySpec(url = \"https://github.com/RoyCCWang/RWPublicJuliaRegistry\")) # where MaternRegression.jl is registered.\nlet\n    pkgs = [\"StaticArrays\", \"MaternRegression\", \"CSV\", \"DataFrames\", \"PythonPlot\",\n    ]\n    for pkg in pkgs\n        #check if package is in current environment.\n        if Base.find_package(pkg) === nothing\n\n            #install package.\n            Pkg.add(pkg)\n        end\n    end\nend\n\nimport Random\nRandom.seed!(25)\n\nusing LinearAlgebra\nusing Statistics\nimport MaternRegression as GS;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"  Activating project at `~/Documents/repo/MaternRegression.jl/examples`\n     Cloning registry from \"https://github.com/RoyCCWang/RWPublicJuliaRegistry\"\nRegistry `RWPublicJuliaRegistry` already exists in `~/.julia/registries/RWPublicJuliaRegistry`.\n","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"When we import Metaheuristics, the add-on module in MaternRegression for hyperparameter optimization also gets loaded.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"import Metaheuristics as EVO;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"for loading the data.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"import CSV\nimport DataFrames as DF\nimport Dates;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Reset plot figures.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"import PythonPlot as PLT\nfig_num = 1;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Specify floating-point data type.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"T = Float64;","category":"page"},{"location":"generated/optim_lit/#Load-data","page":"Hyperparameter optimization","title":"Load data","text":"","category":"section"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Get all the data from the csv into a data frame.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"function get_toronto_station()\n    return \"CA006158665\"\nend\n\nfunction load_data(station_name)\n    data_path = joinpath(\"data\", \"$(station_name).csv\")\n    return CSV.read(data_path, DF.DataFrame)\nend\n\nfunction reverse_standardization(yq::AbstractVector, m, s)\n    return collect(s*yq[n] + m for n in eachindex(yq))\nend\n\ndf_all = load_data(get_toronto_station());","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Remove daily records that have missing maximum temperature.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"df_tmax = filter(xx->!ismissing(xx.TMAX), df_all);","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"The temperature measurements needs to be divided by 10 to get Celcius units.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"N = length(df_tmax.TMAX )\ny0 = collect( convert(T, x/10) for x in df_tmax.TMAX );","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"y is the set of raining outputs.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"mean_y = mean(y0)\nstd_y = std(y0)\ny = (y0 .- mean_y) ./ std_y;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Convert dates to integers, and use as training inputs.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"ts_dates = df_tmax.DATE\nts0 = collect( convert(T,  d |> Dates.value) for d in ts_dates );","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Work with elapsed days as the independent variable. ts is the set of training inputs.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"ts = ts0 .- minimum(ts0)\noffset_date = ts_dates[begin];","category":"page"},{"location":"generated/optim_lit/#Hyperparameter-optimization","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"","category":"section"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"The optimization algorithm tries not to exceed this number of marginal likelihood evaluations:","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"f_calls_limit = 10_000;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"GS.InferGain() constructs a dispatch data type that tells the algorithm that the variable ordering is: [λ, σ², b]","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"trait = GS.InferGain();","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"These are the optimization variable lower and upper bounds, respectively.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"optim_lbs = convert(Vector{T}, [1e-5; 1e-5; 1e-5])\noptim_ubs = convert(Vector{T}, [1.0; 1.0; 1.0]);","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Solve the hyperparameter optimization problem. You need to pass in the module alias EVO so that MaternRegression can check if the pre-requisite dependencies for the hyperparameter optimization package extension can be loaded.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"evo_sol_vars, evo_sol_cost = GS.hp_optim(\n    GS.UseMetaheuristics(EVO),\n    trait,\n    ts, y, optim_lbs, optim_ubs;\n    f_calls_limit = f_calls_limit,\n    #p0s = initial_guesses, #include initial guesses here. These should be of type `Vector{Vector{T}}`.\n);","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"To load results from disk, uncomment the following:","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"##Save to disk, for later use with the QMD file.\n#using Serialization\n#serialize(\"results/optim\", (evo_sol_vars, evo_sol_cost));","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"To load results from disk, uncomment the following:","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"##Load from disk, make sure there were no errors in loading.\n#evo_sol_vars, evo_sol_cost = deserialize(\"results/optim\");","category":"page"},{"location":"generated/optim_lit/#Query-and-visualize","page":"Hyperparameter optimization","title":"Query and visualize","text":"","category":"section"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Choose 5000 uniformly spaced time stamps across the in-fill interval, then pick the first window_len of them as the query positions.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Nq = 5000\nwindow_len = 50\ntqs = LinRange(minimum(ts), maximum(ts), Nq)[1:window_len];","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Parse the hyperparameter optimization results to a Matern kernel, θ_sde_query, and the observation variance, σ².","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"θ_sde_query, σ² = GS.parse_ml_result(trait, evo_sol_vars);","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Run GPR predictive inference to get predictive means mqs and predictive variances vqs.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"sde_gp = GS.create_sdegp(θ_sde_query, ts, y, σ²) # cached phase.\nmqs, vqs = GS.query_sdegp(T, sde_gp, tqs, ts); # query phase.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"The preditive standard deviation.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"sqs = sqrt.(vqs);","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"prepare for display.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"inds = findall(xx->xx<tqs[window_len], ts)\nts_display = ts[inds]\ny_display = reverse_standardization(y[inds], mean_y, std_y)\n\nmqs = reverse_standardization(mqs, mean_y, std_y)\nsqs = sqs .* std_y;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Set the shaded region to be 3 standard deviations from the mean.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"plot_uq_amount = 3 .* sqs;","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"Visualize.","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"fig_size = (6, 4) # units are in inches.\ndpi = 300\n\nPLT.figure(fig_num; figsize = fig_size, dpi = dpi)\nfig_num += 1\n\nPLT.scatter(ts_display, y_display, s = 20, label = \"Data\")\nPLT.plot(tqs, mqs, label = \"Predictive mean\")\n\nPLT.fill_between(\n    tqs,\n    mqs - plot_uq_amount,\n    mqs + plot_uq_amount,\n    alpha = 0.3,\n    label = \"3 standard deviations\"\n)\n\nPLT.xlabel(\"Days elapsed since $offset_date\", fontsize = 12)\nPLT.ylabel(\"Temperature (Celcius)\", fontsize = 12)\nPLT.legend()\nPLT.show() #Not needed if run in the Julia REPL.\nPLT.gcf() #Required for Literate.jl","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"(Image: )","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"","category":"page"},{"location":"generated/optim_lit/","page":"Hyperparameter optimization","title":"Hyperparameter optimization","text":"This page was generated using Literate.jl.","category":"page"}]
}
